"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[580],{6084:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>n,metadata:()=>r,toc:()=>h});var o=a(7624),i=a(2172);const n={sidebar_position:2},s="Introduction",r={id:"Shapley Values/Introduction",title:"Introduction",description:"Shapley value is a second post-hoc explainable AI learning method. As the name suggests, this method is named after the award-winning mathematician and game theorist Lloyd Shapley, who developed the technique back in 1953 in his original papers on cooperative game theory (Shapley et al., 1988). The method develops an explanation for machine learning models through a practical application of cooperative game theory \u2013 that is, the XAI treats each feature of an ML model as a \u201cplayer\u201d, which contributes a value that either adds or subtracts from the average prediction. This value, called the Shapley value, is calculated by looking at all possible coalitions and calculating the average marginal contributions of the given feature (i.e. the difference in predictions with and without the feature). Compared to other XAI methods, particularly LIME, this method guarantees the predictions are fairly distributed and is based on solid theory, but is computationally expensive and requires a lot more data (Molnar, 2023).",source:"@site/docs/Shapley Values/Introduction.md",sourceDirName:"Shapley Values",slug:"/Shapley Values/Introduction",permalink:"/Explainable-Ai-Comps-2024/Shapley Values/Introduction",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Shapley Values/Introduction.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Shapley Values",permalink:"/Explainable-Ai-Comps-2024/category/shapley-values"},next:{title:"Anchors",permalink:"/Explainable-Ai-Comps-2024/category/anchors"}},l={},h=[];function c(e){const t={h1:"h1",p:"p",...(0,i.M)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(t.p,{children:"Shapley value is a second post-hoc explainable AI learning method. As the name suggests, this method is named after the award-winning mathematician and game theorist Lloyd Shapley, who developed the technique back in 1953 in his original papers on cooperative game theory (Shapley et al., 1988). The method develops an explanation for machine learning models through a practical application of cooperative game theory \u2013 that is, the XAI treats each feature of an ML model as a \u201cplayer\u201d, which contributes a value that either adds or subtracts from the average prediction. This value, called the Shapley value, is calculated by looking at all possible coalitions and calculating the average marginal contributions of the given feature (i.e. the difference in predictions with and without the feature). Compared to other XAI methods, particularly LIME, this method guarantees the predictions are fairly distributed and is based on solid theory, but is computationally expensive and requires a lot more data (Molnar, 2023)."}),"\n",(0,o.jsx)(t.p,{children:"There are several reasons why we chose Shapley values as one of our post-hoc methods; in fact, it was the first of the three that we were absolutely sure we wanted to do. This was in large part because of the section by Molnar, suggested to us by Anna during the first week, which did an excellent job of explaining the concept, as well as the advantages and disadvantages of using such a method. Shapley builds on decades-long game theory literature, and since our project requires us to compare three such methods, it seemed it would diversify our analysis."})]})}function d(e={}){const{wrapper:t}={...(0,i.M)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},2172:(e,t,a)=>{a.d(t,{I:()=>r,M:()=>s});var o=a(1504);const i={},n=o.createContext(i);function s(e){const t=o.useContext(n);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(n.Provider,{value:t},e.children)}}}]);