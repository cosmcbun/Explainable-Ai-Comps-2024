"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[472],{4044:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>r,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>h});var n=i(7624),a=i(2172);const o={sidebar_position:1},s="What is XAI?",l={id:"Introduction/What is XAI",title:"What is XAI?",description:"Why should you trust Machine Learning (ML)? Headlines are filled with fundamental flaws in machine learning models published by technology giants.",source:"@site/docs/Introduction/What is XAI.md",sourceDirName:"Introduction",slug:"/Introduction/What is XAI",permalink:"/Explainable-Ai-Comps-2024/Introduction/What is XAI",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Introduction/What is XAI.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/Explainable-Ai-Comps-2024/category/introduction"},next:{title:"Project Overview",permalink:"/Explainable-Ai-Comps-2024/Introduction/Project Overview"}},r={},h=[{value:"Why XAI Matters",id:"why-xai-matters",level:2},{value:"Legal Implications",id:"legal-implications",level:3},{value:"Ethical Implications",id:"ethical-implications",level:3},{value:"XAI for Engineers",id:"xai-for-engineers",level:3},{value:"All of the Techniques illustrated on this Website are <em>Model-Agnostic</em>, <em>Post Hoc</em>, and <em>Local</em>.",id:"all-of-the-techniques-illustrated-on-this-website-are-model-agnostic-post-hoc-and-local",level:2},{value:"What Does Model-Agnostic Mean?",id:"what-does-model-agnostic-mean",level:3},{value:"What is a Post Hoc Explanation/Interpretation?",id:"what-is-a-post-hoc-explanationinterpretation",level:3},{value:"What is a Local Explanaition?",id:"what-is-a-local-explanaition",level:3}];function c(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",strong:"strong",...(0,a.M)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"what-is-xai",children:"What is XAI?"}),"\n",(0,n.jsxs)(t.p,{children:["Why should you trust Machine Learning (ML)? Headlines are filled with ",(0,n.jsx)(t.a,{href:"https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html",children:"fundamental flaws in machine learning models published by technology giants"}),"."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"Accuracy vs explainability graph",src:i(8679).c+"",title:"In general, the more complicated \u2013 and inscrutable \u2013 your model, the more powerful it can become.",width:"850",height:"567"})}),"\n",(0,n.jsx)(t.p,{children:"While they do exist, most real world applications lean away from inherently explainable models like decision trees or linear regression models, and towards black box models like neural networks due to their complex and robust hypothesis spaces. This flexibility allows them to perform better in most cases of complex relationships. However, it also engenders a need for explanations."}),"\n",(0,n.jsx)(t.p,{children:"This page explores the need for explainable AI (XAI) techniques. Explainable AI techniques attempt to illustrate how a machine learning model comes to a conclusion."}),"\n",(0,n.jsx)(t.h2,{id:"why-xai-matters",children:"Why XAI Matters"}),"\n",(0,n.jsxs)(t.p,{children:["We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences \u2014 namely their predictions. As discussed in Ribeiro et al. ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/1602.04938",children:"(2016, p.1)"}),", the act of explaining an AI\u2019s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users\u2019 trust in the model and exposing any possible errors in the model\u2019s structure."]}),"\n",(0,n.jsx)(t.h3,{id:"legal-implications",children:"Legal Implications"}),"\n",(0,n.jsxs)(t.p,{children:["With regulations like the EU\u2019s ",(0,n.jsx)(t.a,{href:"https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai",children:"Right to Explainability"})," and the United States\u2019 proposed ",(0,n.jsx)(t.a,{href:"https://www.whitehouse.gov/ostp/ai-bill-of-rights/",children:"AI Bill of Rights"}),", a machine learning model may no longer be a simple \u201cblack box\u201d: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche."]}),"\n",(0,n.jsxs)(t.p,{children:["In such an impossibly dense field, how can one quantify a method\u2019s efficacy? ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/Shapley%20Values/The%20EU's%20right%20to%20explainability",children:"Which method would a jury trust?"})]}),"\n",(0,n.jsx)(t.h3,{id:"ethical-implications",children:"Ethical Implications"}),"\n",(0,n.jsxs)(t.p,{children:["Although Artificial Intelligence has been used across many fields for a long time, advancements in the past few decades have led to organizations implementing them more frequently in load-bearing systems. One simple example of this ",(0,n.jsx)(t.a,{href:"https://github.com/features/copilot",children:"Github's Copilot"}),", which has streamlined development worldwide but often adheres to low-quality and insecure coding practices, as noted in ",(0,n.jsx)(t.a,{href:"https://nordvpn.com/blog/is-github-copilot-safe-to-use-at-work/",children:"Ugn\u0117 Zieni\u016bt\u0117's 2023 analysis of the tool"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["However, this is not the only domain to which AI is being applied. ",(0,n.jsx)(t.strong,{children:"High-risk systems are beginning to adapt AI into their core operations."})," As described in ",(0,n.jsx)(t.a,{href:"https://journals.lww.com/smj/Fulltext/2023/02000/Machine_learning_in_medicine__what_clinicians.1.aspx#:~:text=on%20ML%20projects.-,Explainability,-The%20concept%20of",children:"Sim et al. (2023)"}),", machine learning models can help to greatly expedite delivery of care, but the lack of clarity in their calculations can lead not only to incorrect diagnoses, but also to distrust in the medical system. Moreover, titan of the investment industry S&P GLobal predicts that AI will soon automate risk management and profit-optimization ",(0,n.jsx)(t.a,{href:"https://www.spglobal.com/en/research-insights/featured/special-editorial/ai-in-banking-ai-will-be-an-incremental-game-changer",children:"(Fern\xe1ndez, 2023)"}),". ",(0,n.jsx)(t.strong,{children:"How do we know what these models consider a risk? How can we be sure a profit optimization is feasible and ethical?"})]}),"\n",(0,n.jsx)(t.h3,{id:"xai-for-engineers",children:"XAI for Engineers"}),"\n",(0,n.jsxs)(t.p,{children:["Explainable AI techniques provide a window into the ML black box which allow us to make much more specific improvements to our models. We can uncover unforeseen biases, oversights, and and potential ethical/legal concerns, all through a few lines of code. ",(0,n.jsx)(t.strong,{children:"With XAI, you can not only build AI that you and your users can understand, but you can build a model that you can trust."})]}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsxs)(t.h2,{id:"all-of-the-techniques-illustrated-on-this-website-are-model-agnostic-post-hoc-and-local",children:["All of the Techniques illustrated on this Website are ",(0,n.jsx)(t.em,{children:"Model-Agnostic"}),", ",(0,n.jsx)(t.em,{children:"Post Hoc"}),", and ",(0,n.jsx)(t.em,{children:"Local"}),"."]}),"\n",(0,n.jsx)(t.h3,{id:"what-does-model-agnostic-mean",children:"What Does Model-Agnostic Mean?"}),"\n",(0,n.jsx)(t.p,{children:"Usage of a model-agnostic technique does not depend on knowledge of the specific AI model being analyzed. This means that, rather than looking at the specific activations of the network, or the weights and biases, we only have access to the outputs of the model. This may also be described as a black-box approach."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"Black Box Clipart",src:i(8868).c+"",title:"What is happening in the box? Not our problem!",width:"910",height:"380"})}),"\n",(0,n.jsxs)(t.p,{children:["There are a few reasons that this might be desirable. For instance, work done on one model can scale easily to others. With the same techniques in this project, we were able to analyze both ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/User%20Study/ResNet%20-%20Comparative%20Results",children:"images"})," and ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/User%20Study/MOOC%20-%20Comparative%20Results",children:"tabular data"}),"! Moreover, it allows us to gather insights into models we don't have unrestricted access to. Given that AI model weights are often a valuable property that companies have spent much money and time on tuning, they probably won't be willing to share the weights. But model-agnostic techniques only need to be able to query the model repeatedly."]}),"\n",(0,n.jsx)(t.h3,{id:"what-is-a-post-hoc-explanationinterpretation",children:"What is a Post Hoc Explanation/Interpretation?"}),"\n",(0,n.jsx)(t.p,{children:"Post hoc explanations attempt to answer, in retrospect, why the model chose the output or classification it did. It's beyond the intended scope of these techniques to predict what the model will say in future instances without testing it on those as well."}),"\n",(0,n.jsx)(t.h3,{id:"what-is-a-local-explanaition",children:"What is a Local Explanaition?"}),"\n",(0,n.jsx)(t.p,{children:"These techniques are not focused on the behavior of the AI model as a whole, but rather on its behavior in a single instance \u2013 one data point. The conclusions, thusly, are highly centered to the one prediction, which can give the user much more specific insight into one occurrence of a model's thoguht process; however, these methods cannot necessarily be extrapolated too far in any direction."}),"\n",(0,n.jsxs)(t.p,{children:["This peeks out from under the umbrella of ",(0,n.jsx)(t.strong,{children:"Interpretable AI"}),", which seeks to explain more generally how an AI comes to its conclusions. These can include global explanations of how a model's processes work, but are sometimes more lossy than local explanations."]}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"shap"})," package, which we use to implement one of our techniques, is able to perform ",(0,n.jsx)(t.strong,{children:"Cohort explanations"}),", which lies between local and global explanation ",(0,n.jsx)(t.a,{href:"https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f",children:"(Dhinakaran, 2021)"})," (see ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/Shapley%20Values/Shapley%20and%20MOOC",children:"Shapley and MOOC"}),")."]})]})}function d(e={}){const{wrapper:t}={...(0,a.M)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8868:(e,t,i)=>{i.d(t,{c:()=>n});const n=i.p+"assets/images/Black_Box_Clipart-a5cb5e5cb2fc7a4a357f0d8e8523b664.png"},8679:(e,t,i)=>{i.d(t,{c:()=>n});const n=i.p+"assets/images/accuracy_vs_explainability-22fa580681a469e43827f024698ae327.png"},2172:(e,t,i)=>{i.d(t,{I:()=>l,M:()=>s});var n=i(1504);const a={},o=n.createContext(a);function s(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);