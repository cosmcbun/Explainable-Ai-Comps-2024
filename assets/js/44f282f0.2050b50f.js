"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[1306],{516:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>n,metadata:()=>o,toc:()=>h});var i=s(7624),a=s(2172);const n={sidebar_position:2},l="MOOC Dropout Prediction",o={id:"LIME/MOOC",title:"MOOC Dropout Prediction",description:"Summary",source:"@site/docs/LIME/MOOC.md",sourceDirName:"LIME",slug:"/LIME/MOOC",permalink:"/Explainable-Ai-Comps-2024/LIME/MOOC",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/LIME/MOOC.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/Explainable-Ai-Comps-2024/LIME/Introduction"},next:{title:"ResNet Image Classification",permalink:"/Explainable-Ai-Comps-2024/LIME/ResNet"}},r={},h=[{value:"Summary",id:"summary",level:2},{value:"Apply LIME to MOOCs",id:"apply-lime-to-moocs",level:2},{value:"Parsing the Output",id:"parsing-the-output",level:2},{value:"LIME Specific Output",id:"lime-specific-output",level:3},{value:"LIME Output used in User Study",id:"lime-output-used-in-user-study",level:3},{value:"User Study",id:"user-study",level:2}];function c(e){const t={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",math:"math",mn:"mn",mo:"mo",mrow:"mrow",mtext:"mtext",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",ul:"ul",...(0,a.M)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"mooc-dropout-prediction",children:"MOOC Dropout Prediction"}),"\n",(0,i.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(t.p,{children:"LIME was trivial to apply to the MOOC Model, provided inelegant but effective explanation visuals, and there appeared to be a positive relation between a viewer's experience with ML and their belief that LIME explanation helped explain the model."}),"\n",(0,i.jsx)(t.h2,{id:"apply-lime-to-moocs",children:"Apply LIME to MOOCs"}),"\n",(0,i.jsx)(t.p,{children:"Applying LIME to the ML model used to predict if a student would drop out of a MOOC straight forward and did not require a background in explainable AI techniques."}),"\n",(0,i.jsxs)(t.p,{children:["The literature has centralized around one implementation, the Python package ",(0,i.jsx)(t.a,{href:"https://pypi.org/project/lime/",children:"LIME"}),", which has extensive and easy to use documentation."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-Python",children:"from lime.lime_tabular import LimeTabularExplainer\n\n# Create LimeTabularExplainer\nsvm_explainer = LimeTabularExplainer(\n    X_train, # training data created by the MOOC src class\n    feature_names=[\n        'viewed', 'gender','grade','nevents', 'ndays_act',\n        'nplay_video', 'nchapters', 'age', 'votes', 'num_words'\n    ],\n    class_names=['Not Completed','Completed'],\n    discretize_continuous=True\n)\n\n# Use the explainer to graphically explain predictions\nexp = svm_explainer.explain_instance(\n    dummy_for_lime, # a fake row of data for LIME to explain\n    svc.predict_proba # the black box model's probabilistic prediction method\n).show_in_notebook(\n    show_table=True,\n    show_all=False\n)\n"})}),"\n",(0,i.jsxs)(t.p,{children:["The above code block is a simplified version of ",(0,i.jsx)(t.a,{href:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/blob/62e136607f3b66106fd09fd558feb38f4834419a/MOOC/LIME/BasicExampleOfUsingLIMEOnMOOCDataset.ipynb",children:"Our Implementation"})," which outputs an explanation similar to the visualization seen below."]}),"\n",(0,i.jsx)(t.h2,{id:"parsing-the-output",children:"Parsing the Output"}),"\n",(0,i.jsx)(t.h3,{id:"lime-specific-output",children:"LIME Specific Output"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"LIME Specific Output",src:s(3716).c+"",width:"2937",height:"1182"})}),"\n",(0,i.jsxs)(t.p,{children:["The left third, while created by the LIME package, is not XAI. The visualization simply illustrates the black box model\u2019s prediction which LIME is attempting to explain. The table could be read as \u201cThe black box model is 28% confident that the student of interest will not complete the course ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsxs)(t.mrow,{children:[(0,i.jsx)(t.mtext,{children:"\u2005\u200a"}),(0,i.jsx)(t.mo,{children:"\u27f9"}),(0,i.jsx)(t.mtext,{children:"\u2005\u200a"})]}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\implies"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.549em",verticalAlign:"-0.024em"}}),(0,i.jsx)(t.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u27f9"}),(0,i.jsx)(t.span,{className:"mspace",style:{marginRight:"0.2778em"}})]})})]})," The black box model is 72% confident that the student of interest will complete the course.\u201d"]}),"\n",(0,i.jsx)(t.p,{children:"The right third, displays the feature values of the input which LIME is attempting to explain. LIME believes orange features encourage the black box model to predict that the student will not complete the course. Similarly, LIME explains that blue features pushed the black box model to conclude that the student would not complete the course. The higher in the table the greater the contribution to the final prediction. The first row of this table could be read as \u201cBecause the number of events the student of interest has participated in is 32,296, the black box model is more confident that the student will complete the course.\u201d"}),"\n",(0,i.jsx)(t.p,{children:"The center visual displays the relative importance of each feature and why a feature provides a positive or negative contribution to the final prediction. The larger the bars, the more important the feature is in the final decision. Each bar is accompanied with a rule. The rule explains how/why the feature value contributes to the final prediction. The first line could be read as \u201cBecause the student of interest has completed more than 203 events, the model is substantially more confident that the student will complete the course.\u201d"}),"\n",(0,i.jsx)(t.h3,{id:"lime-output-used-in-user-study",children:"LIME Output used in User Study"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"LIME Output used in User Study",src:s(6292).c+"",width:"794",height:"435"})}),"\n",(0,i.jsx)(t.p,{children:"This output, while not the default output of the LIME package, is one of the pre-made visualizations available through the package."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"The x-axes represents the probability that the student of interest has completed the course."}),"\n",(0,i.jsx)(t.li,{children:"The bars starting at the y-axes represent each features contribution to the black box models prediction."}),"\n",(0,i.jsx)(t.li,{children:"If the sum of all bars (where red bars are negative) is greater than 0.5, then the black box model will predict that the student completed the course. Otherwise, the black box model will predict that the student did not complete the course."}),"\n",(0,i.jsx)(t.li,{children:"The y-axes labels are rules as to why each feature pushed or pulled the final prediction."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"We used this visualization for the user study despite this not being the default visualization from the LIME package because:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"It was much more visually similar to Shapley."}),"\n",(0,i.jsx)(t.li,{children:"Where the default output provides information unrelated to XAI, this output only outputs novel information resulting from XAI processes."}),"\n",(0,i.jsx)(t.li,{children:"This output helps participants focus on the rules based portion (the y-axis labels) helping connect Anchors\u2019 output to LIME\u2019s."}),"\n",(0,i.jsx)(t.li,{children:"While testing if summing each bar of either visualization is greater than 0.5 can be done, we believe this operation is more intuitive with the second visualization."}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"user-study",children:"User Study"}),"\n",(0,i.jsx)(t.p,{children:"Consider reading the user study introduction and methodology before continuing"}),"\n",(0,i.jsxs)(t.p,{children:["Little can be said about LIME's user study results in isolation. To see how LIME compares to Shapley and Anchors, see the ",(0,i.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/User%20Study/MOOC%20-%20Comparative%20Results",children:"MOOC Comparative User Study"})," page."]}),"\n",(0,i.jsxs)(t.p,{children:["Across all 5 samples, participants ranked LIME explanation understandability as ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsxs)(t.mrow,{children:[(0,i.jsx)(t.mo,{children:"\u2248"}),(0,i.jsx)(t.mn,{children:"4"})]}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\approx4"})]})})}),(0,i.jsxs)(t.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.4831em"}}),(0,i.jsx)(t.span,{className:"mrel",children:"\u2248"}),(0,i.jsx)(t.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(t.span,{className:"mord",children:"4"})]})]})]})," on average ",(0,i.jsxs)(t.span,{className:"katex",children:[(0,i.jsx)(t.span,{className:"katex-mathml",children:(0,i.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(t.semantics,{children:[(0,i.jsxs)(t.mrow,{children:[(0,i.jsx)(t.mo,{children:"\xb1"}),(0,i.jsx)(t.mn,{children:"1"})]}),(0,i.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\pm 1"})]})})}),(0,i.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(t.span,{className:"base",children:[(0,i.jsx)(t.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(t.span,{className:"mord",children:"\xb1"}),(0,i.jsx)(t.span,{className:"mord",children:"1"})]})})]}),"."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Sample All LIME - How Does Experience with ML Affect Model Explainability.png",src:s(6564).c+"",width:"1500",height:"500"})}),"\n",(0,i.jsx)(t.p,{children:"Across all 5 samples, the greater the participant's experience with ML, the more they believed LIME explanations helped explain the model."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Sample All LIME - How Does Experience with ML Affect Model Understandability",src:s(6324).c+"",width:"1500",height:"500"})}),"\n",(0,i.jsx)(t.p,{children:"Without a greater sample size, it is difficult to draw any conclusions from this graph. However, it is interesting to note that participants with the least and most experience with ML had the highest confidence that they understood the MOOC model as explained by LIME."})]})}function d(e={}){const{wrapper:t}={...(0,a.M)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},3716:(e,t,s)=>{s.d(t,{c:()=>i});const i=s.p+"assets/images/HTML Output-0e682c0abab400274c67f3b6dc4900ed.png"},6564:(e,t,s)=>{s.d(t,{c:()=>i});const i=s.p+"assets/images/Sample All LIME - How Does Experience with ML Affect Model Explainability-a9ac24ed620ac40f9f7206863b4ff437.png"},6324:(e,t,s)=>{s.d(t,{c:()=>i});const i=s.p+"assets/images/Sample All LIME - How Does Experience with ML Affect Model Understandability-630211bb6f0560572142f35aab321897.png"},6292:(e,t,s)=>{s.d(t,{c:()=>i});const i=s.p+"assets/images/X_complete3-4679e47e2bfe0856de110c39cdf03602.png"},2172:(e,t,s)=>{s.d(t,{I:()=>o,M:()=>l});var i=s(1504);const a={},n=i.createContext(a);function l(e){const t=i.useContext(n);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(n.Provider,{value:t},e.children)}}}]);