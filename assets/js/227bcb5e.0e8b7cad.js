"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[1352],{7236:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>h,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var a=i(7624),n=i(2172);const s={sidebar_position:6},o=void 0,r={id:"User Study/Quote_Takeaways",title:"Quote_Takeaways",description:"Resnet Participant Comments",source:"@site/docs/User Study/Quote_Takeaways.md",sourceDirName:"User Study",slug:"/User Study/Quote_Takeaways",permalink:"/Explainable-Ai-Comps-2024/User Study/Quote_Takeaways",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/User Study/Quote_Takeaways.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Limitations",permalink:"/Explainable-Ai-Comps-2024/User Study/Limitations"},next:{title:"MOOC: Comparative Results",permalink:"/Explainable-Ai-Comps-2024/User Study/MOOC - Comparative Results"}},h={},l=[{value:"Resnet Participant Comments",id:"resnet-participant-comments",level:2},{value:"LIME and Anchoring more intuitive...",id:"lime-and-anchoring-more-intuitive",level:3},{value:"but Shapley still preferred",id:"but-shapley-still-preferred",level:3},{value:"Clashing explanations",id:"clashing-explanations",level:3}];function p(e){const t={h2:"h2",h3:"h3",p:"p",...(0,n.M)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"resnet-participant-comments",children:"Resnet Participant Comments"}),"\n",(0,a.jsx)(t.h3,{id:"lime-and-anchoring-more-intuitive",children:"LIME and Anchoring more intuitive..."}),"\n",(0,a.jsx)(t.p,{children:"Multiple participants noted that LIME and Anchoring produced more intuitive explanations than Shapley. One participant said \"I can kind of see a snake in Shapley but there is no 'oh okay' moment like LIME has\" about the explanations below of the model's prediction of horned viper.\n(horned viper)"}),"\n",(0,a.jsx)(t.p,{children:'Another participant said about the image of the cat with the roses that they "liked LIME and Anchoring more because it is possible to see the image", which contrasts with the heat map in Shapley partially obscuring the images it is explaining, as a participant also noted "it\'s hard to see the pixels in Shapley" about the image with the cat in the box.\n(roses)\n(pug)'}),"\n",(0,a.jsx)(t.h3,{id:"but-shapley-still-preferred",children:"but Shapley still preferred"}),"\n",(0,a.jsx)(t.p,{children:'Despite this, (9 or 10 out of 12) of participants still named Shapley as their favorite technique. One reason was that it was more likely to highlight only the relevant parts of the image. Regarding the explanations for the picture of the cat on the stairs, one participant said "highlighting the stairs is a little weird in LIME and Anchoring", and another participant pointed out that "Shapley makes completely clear [the model] is looking at the ball" about the picture of the pug with the tennis ball, which contrasts with LIME and Anchoring highlighting much of its face.\n(stairs)'}),"\n",(0,a.jsx)(t.p,{children:'When comparing all of the techniques after seeing all of the examples, one participant said "Shapley was the most consistent". Another upside of Shapley was its ability to capture values, with different participants saying "I like Shapley because it reflects degrees of importance", and "big fan of heat map, it provides the most helpful information".'}),"\n",(0,a.jsx)(t.h3,{id:"clashing-explanations",children:"Clashing explanations"}),"\n",(0,a.jsx)(t.p,{children:"One phenomenon that confused participants was when the techniques gave contradictory explanations. One example of this is the picture of a dog that the model thought was a polar bear."}),"\n",(0,a.jsx)(t.p,{children:"(polar bear)"}),"\n",(0,a.jsx)(t.p,{children:'LIME focused on the dog\'s head, whereas Anchoring included most of the dog and much of the background but omitted much of the head, and Shapley highlighted some of the head red and some blue. All explanations are plausible in isolation, but one participant said "If I was given any single one I would say they help, but they confuse in total". After seeing all of the examples, another participant said "it is actively confusing to have different explanations for the same model. If they are seeing the same predictions, they should be showing the same explanation."'})]})}function c(e={}){const{wrapper:t}={...(0,n.M)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},2172:(e,t,i)=>{i.d(t,{I:()=>r,M:()=>o});var a=i(1504);const n={},s=a.createContext(n);function o(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);