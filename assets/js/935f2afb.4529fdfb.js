"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[5696],{5988:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"What is XAI?","href":"/Explainable-Ai-Comps-2024/Introduction/What is XAI","docId":"Introduction/What is XAI","unlisted":false},{"type":"link","label":"Project Overview","href":"/Explainable-Ai-Comps-2024/Introduction/Project Overview","docId":"Introduction/Project Overview","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/introduction"},{"type":"category","label":"Methodology","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"MOOC Dropout Prediction","href":"/Explainable-Ai-Comps-2024/Methodology/MOOC","docId":"Methodology/MOOC","unlisted":false},{"type":"link","label":"ResNet Animal Classification","href":"/Explainable-Ai-Comps-2024/Methodology/ResNet","docId":"Methodology/ResNet","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/methodology"},{"type":"category","label":"LIME","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Explainable-Ai-Comps-2024/LIME/Introduction","docId":"LIME/Introduction","unlisted":false},{"type":"link","label":"MOOC Dropout Prediction","href":"/Explainable-Ai-Comps-2024/LIME/MOOC","docId":"LIME/MOOC","unlisted":false},{"type":"link","label":"ResNet Image Classification","href":"/Explainable-Ai-Comps-2024/LIME/ResNet","docId":"LIME/ResNet","unlisted":false},{"type":"link","label":"LIME with the Cats and Dogs Dataset","href":"/Explainable-Ai-Comps-2024/LIME/Animals","docId":"LIME/Animals","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/lime"},{"type":"category","label":"Shapley Values","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Explainable-Ai-Comps-2024/Shapley Values/Introduction","docId":"Shapley Values/Introduction","unlisted":false},{"type":"link","label":"Shapley and MOOC","href":"/Explainable-Ai-Comps-2024/Shapley Values/Shapley and MOOC","docId":"Shapley Values/Shapley and MOOC","unlisted":false},{"type":"link","label":"Applying Shapley to the ResNet network","href":"/Explainable-Ai-Comps-2024/Shapley Values/Shapley and Resnet","docId":"Shapley Values/Shapley and Resnet","unlisted":false},{"type":"link","label":"Shapley\'s Math","href":"/Explainable-Ai-Comps-2024/Shapley Values/Shapley\'s Math","docId":"Shapley Values/Shapley\'s Math","unlisted":false},{"type":"link","label":"The EU\'s Right to Explainability","href":"/Explainable-Ai-Comps-2024/Shapley Values/The EU\'s right to explainability","docId":"Shapley Values/The EU\'s right to explainability","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/shapley-values"},{"type":"category","label":"Anchors","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"What is Anchoring","href":"/Explainable-Ai-Comps-2024/Anchors/Introduction","docId":"Anchors/Introduction","unlisted":false},{"type":"link","label":"Anchors on MOOC","href":"/Explainable-Ai-Comps-2024/Anchors/MOOC-Anchors","docId":"Anchors/MOOC-Anchors","unlisted":false},{"type":"link","label":"Tumors","href":"/Explainable-Ai-Comps-2024/Anchors/Tumors","docId":"Anchors/Tumors","unlisted":false},{"type":"link","label":"Animals","href":"/Explainable-Ai-Comps-2024/Anchors/Animals","docId":"Anchors/Animals","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/anchors"},{"type":"link","label":"Tumors Case Study","href":"/Explainable-Ai-Comps-2024/Tumors","docId":"Tumors","unlisted":false},{"type":"category","label":"User Study / Results","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Explainable-Ai-Comps-2024/User Study/Introduction","docId":"User Study/Introduction","unlisted":false},{"type":"link","label":"Limitations","href":"/Explainable-Ai-Comps-2024/User Study/Limitations","docId":"User Study/Limitations","unlisted":false},{"type":"link","label":"Qualitative Takeaways","href":"/Explainable-Ai-Comps-2024/User Study/ResNet - Qualitative Takeaways","docId":"User Study/ResNet - Qualitative Takeaways","unlisted":false},{"type":"link","label":"MOOC: Comparative Results","href":"/Explainable-Ai-Comps-2024/User Study/MOOC - Comparative Results","docId":"User Study/MOOC - Comparative Results","unlisted":false},{"type":"link","label":"MOOC: Methodology","href":"/Explainable-Ai-Comps-2024/User Study/MOOC - Methodology","docId":"User Study/MOOC - Methodology","unlisted":false},{"type":"link","label":"ResNet: Comparative Results","href":"/Explainable-Ai-Comps-2024/User Study/ResNet - Comparative Results","docId":"User Study/ResNet - Comparative Results","unlisted":false},{"type":"link","label":"ResNet: Methodology","href":"/Explainable-Ai-Comps-2024/User Study/ResNet - Methodology","docId":"User Study/ResNet - Methodology","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/user-study--results"},{"type":"category","label":"Extensions","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Tumors Case Study","href":"/Explainable-Ai-Comps-2024/Extensions/Tumors","docId":"Extensions/Tumors","unlisted":false},{"type":"link","label":"Adversarial Explanations","href":"/Explainable-Ai-Comps-2024/Extensions/XAI+AdvAI","docId":"Extensions/XAI+AdvAI","unlisted":false}],"href":"/Explainable-Ai-Comps-2024/category/extensions"},{"type":"link","label":"Posters","href":"/Explainable-Ai-Comps-2024/Posters","docId":"Posters","unlisted":false},{"type":"link","label":"About Us","href":"/Explainable-Ai-Comps-2024/About Us","docId":"About Us","unlisted":false},{"type":"link","label":"Bibliography","href":"/Explainable-Ai-Comps-2024/Citations","docId":"Citations","unlisted":false}]},"docs":{"About Us":{"id":"About Us","title":"About Us","description":"=======","sidebar":"tutorialSidebar"},"Anchors/Animals":{"id":"Anchors/Animals","title":"Animals","description":"Anchoring Successes","sidebar":"tutorialSidebar"},"Anchors/Introduction":{"id":"Anchors/Introduction","title":"What is Anchoring","description":"Ribeiro, Singh, and Guestrin, the original authors of LIME, also created anchoring as an Explainable AI model, which has some similarities with LIME, but outputs its explanations in a different form. Like LIME, Anchoring involves perturbing the data point in question to see how the results from the black box change. Ribeiro et al. define their anchor like so: \u201cAn anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally \u2013 such that changes to the rest of the feature values of the instance do not matter\u201d (Ribeiro et al, 2018). For example, a picture of a dog in the ocean would still be a picture of a dog even if the background were changed to grass. In this case, the dog would be our anchor. The same principle can be used for multiple types of data, not just image data. For tabular data, the algorithm will seek to find which feature values were the most important in coming to a particular decision, and for image classification the anchor will be a set of superpixels that have the most importance in determining the prediction of the model. A clear advantage of anchoring is that its output is intuitive and easy to interpret, as opposed to being a sea of coefficients.","sidebar":"tutorialSidebar"},"Anchors/MOOC-Anchors":{"id":"Anchors/MOOC-Anchors","title":"Anchors on MOOC","description":"Introduction","sidebar":"tutorialSidebar"},"Anchors/Tumors":{"id":"Anchors/Tumors","title":"Tumors","description":"XAI in this case is meant as a way of highlighting tumors in brain scans. If a model predicts that an image has a tumor, then ideally our techniques would find and highlight the tumor as the reason the model made its prediction.","sidebar":"tutorialSidebar"},"Citations":{"id":"Citations","title":"Bibliography","description":"Artificial Intelligence Act//www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai","sidebar":"tutorialSidebar"},"Extensions/Tumors":{"id":"Extensions/Tumors","title":"Tumors Case Study","description":"Introduction","sidebar":"tutorialSidebar"},"Extensions/XAI+AdvAI":{"id":"Extensions/XAI+AdvAI","title":"Adversarial Explanations","description":"Although Explainable AI can exist in a vacuum, it does not have to. As noted in our introduction, XAI can be used to motivate and direct the design of models that users can trust it strives to \\"cause the black box to break down.\\" Both of these approaches to AI share a common goal: to construct models that are more robust against variations in data, biases, and even active exploitation.","sidebar":"tutorialSidebar"},"Introduction/Project Overview":{"id":"Introduction/Project Overview","title":"Project Overview","description":"As a 2024 Comps/Capstone for Carleton College\'s Computer Science Department, this project has two parallel tracks following the literature review: appling XAI techniques to tabular categorization problems and appling XAI techniques to image categorization problems.","sidebar":"tutorialSidebar"},"Introduction/What is XAI":{"id":"Introduction/What is XAI","title":"What is XAI?","description":"Why should you trust Machine Learning (ML)? Headlines are filled with fundamental flaws in machine learning models published by technology giants.","sidebar":"tutorialSidebar"},"LIME/Animals":{"id":"LIME/Animals","title":"LIME with the Cats and Dogs Dataset","description":"Cartons and Vipers and Bears, oh my!","sidebar":"tutorialSidebar"},"LIME/Introduction":{"id":"LIME/Introduction","title":"Introduction","description":"Local Interpretable Model-Agnostic Explanations (LIME)","sidebar":"tutorialSidebar"},"LIME/MOOC":{"id":"LIME/MOOC","title":"MOOC Dropout Prediction","description":"Summary","sidebar":"tutorialSidebar"},"LIME/ResNet":{"id":"LIME/ResNet","title":"ResNet Image Classification","description":"Residual Neural Network","sidebar":"tutorialSidebar"},"Methodology/MOOC":{"id":"Methodology/MOOC","title":"MOOC Dropout Prediction","description":"The MOOC Dataset","sidebar":"tutorialSidebar"},"Methodology/ResNet":{"id":"Methodology/ResNet","title":"ResNet Animal Classification","description":"This project\u2019s first model of focus is an implementation of the Residual Neural Network (ResNet) architecture, a derivative of the basic convolutional neural network. This network was trained on ImageNet, an extremely common dataset for image classification, and will be used in conjunction with a dataset for tumor classification such as that of Panigrahi (2021) and Sarta (2020). Coined by He et al. (2015), ResNet addresses the issues of neural networks\u2019 degradation, where training and evaluation losses spike and lose predictability as a model\u2019s depth passes a certain point. The proposed solution consists of identity mapping, whereby layers pass their outputs to a mapping function that performs one of two actions: it may shortcut part of the input to be recombined at a deeper layer (discussed in Hochreiter et al., 1997), or it may utilize a nonlinear function which asymptotically approximated the use of multiple linear layers at once (introduced in J\xe9gou et al., 2012). This development allows deep neural networks to have no worse loss than shallow ones of appropriate depth.","sidebar":"tutorialSidebar"},"Posters":{"id":"Posters","title":"Posters","description":"Adrian Boskovic","sidebar":"tutorialSidebar"},"Shapley Values/Introduction":{"id":"Shapley Values/Introduction","title":"Introduction","description":"Shapley values offer retroactive local explanations of an AI\'s decisions. As the name suggests, this technique is named after the award-winning mathematician and computational theorist Lloyd Shapley, who developed it back in the 1950s in his original papers on cooperative game theory (Shapley et al., 1952). The technique derives an explanation for machine learning models through a practical application of cooperative game theory \u2013 that is, the XAI treats each feature of an ML model as a \u201cplayer\u201d, which contributes a value that either adds or subtracts from the average prediction. This value, called the Shapley value, is calculated by looking at all possible coalitions and calculating the average marginal contributions of the given feature (i.e. the difference in predictions with and without the feature). Compared to other XAI techniques, particularly LIME, Shapley\'s method guarantees the predictions are fairly distributed and is based on solid theory, but is computationally expensive and, in some cases, may require access to the model\'s training data (Molnar, 2023).","sidebar":"tutorialSidebar"},"Shapley Values/Shapley and MOOC":{"id":"Shapley Values/Shapley and MOOC","title":"Shapley and MOOC","description":"This section discusses the application of the `shap` package to the Multi-Layer Perceptron that we built for the MOOC dataset. For image data, please refer to Shapley and ResNet.","sidebar":"tutorialSidebar"},"Shapley Values/Shapley and Resnet":{"id":"Shapley Values/Shapley and Resnet","title":"Applying Shapley to the ResNet network","description":"This section discusses the application of the `shap` package to the standard image-recognition architecture ResNet - both a standard version and a modified one we trained on brain tumor images.","sidebar":"tutorialSidebar"},"Shapley Values/Shapley\'s Math":{"id":"Shapley Values/Shapley\'s Math","title":"Shapley\'s Math","description":"Due to their strong mathematical backing, Shapley values are incredibly widely used in the field, thus they are almost obligatory to include in the project. But how do Shapley values work?","sidebar":"tutorialSidebar"},"Shapley Values/The EU\'s right to explainability":{"id":"Shapley Values/The EU\'s right to explainability","title":"The EU\'s Right to Explainability","description":"(Disclaimer: none on this team are certified lawyers. This is an exploration of Shapley as a concept and is not legal advice)","sidebar":"tutorialSidebar"},"Tumors":{"id":"Tumors","title":"Tumors Case Study","description":"Introduction","sidebar":"tutorialSidebar"},"User Study/Introduction":{"id":"User Study/Introduction","title":"Introduction","description":"Two user studies were run. Each study consisted of one on one interviews with around 12 participants and took around 20 minutes to complete. Both studies shared the same introduction to the concept of XAI, and conclusion which gauge participant trust and preference for each model. The goals of the study was to:","sidebar":"tutorialSidebar"},"User Study/Limitations":{"id":"User Study/Limitations","title":"Limitations","description":"MOOC Survey Limitations","sidebar":"tutorialSidebar"},"User Study/MOOC - Comparative Results":{"id":"User Study/MOOC - Comparative Results","title":"MOOC: Comparative Results","description":"\\"It really breaks down the idea of the black box model.\\"","sidebar":"tutorialSidebar"},"User Study/MOOC - Methodology":{"id":"User Study/MOOC - Methodology","title":"MOOC: Methodology","description":"A user study was designed around these explainability methods, which set out to assess these methods in a variety of ways. Loosely modeled after the anchoring paper (Ribeiro 2016), the first component exposes respondents to a data point, and one explanatory method. To assess the explanatory power of the method, the respondent\u2019s accuracy at predicting the model\u2019s outcome in subsequent data points is assessed. If the explanation was effective at helping the respondent understand how the model makes its predictions, they should increase the respondent\u2019s ability to accurately predict the model. Following exposure to all the methods across a variety of samples, respondents provide feedback on each.","sidebar":"tutorialSidebar"},"User Study/ResNet - Comparative Results":{"id":"User Study/ResNet - Comparative Results","title":"ResNet: Comparative Results","description":"Summary","sidebar":"tutorialSidebar"},"User Study/ResNet - Methodology":{"id":"User Study/ResNet - Methodology","title":"ResNet: Methodology","description":"We designed a user study to determine how explanatory \u2013 and convincing \u2013\xa0our explanations of ResNet predictions were to users. Here is how we conducted this half of the study.","sidebar":"tutorialSidebar"},"User Study/ResNet - Qualitative Takeaways":{"id":"User Study/ResNet - Qualitative Takeaways","title":"Qualitative Takeaways","description":"ResNet Participant Comments","sidebar":"tutorialSidebar"}}}')}}]);