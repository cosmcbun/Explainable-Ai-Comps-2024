"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[8424],{860:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>l,frontMatter:()=>a,metadata:()=>i,toc:()=>h});var o=s(7624),n=s(2172);const a={},r="User Study Overview",i={id:"Methodology/User-Study-intro",title:"User Study Overview",description:"A user study was designed around these explainability methods, which set out to assess these methods in a variety of ways. Loosely modeled after the anchoring paper (Ribeiro, 2016), the first component exposes respondents to a data point, and one explanatory method. To assess the explanatory power of the method, the respondent\u2019s accuracy at predicting the model\u2019s outcome in subsequent data points is assessed. If the explanation was effective at helping the respondent understand how the model makes its predictions, they should increase the respondent\u2019s ability to accurately predict the model. Following exposure to all the methods across a variety of samples, respondents provide feedback on each.",source:"@site/docs/Methodology/User-Study-intro.md",sourceDirName:"Methodology",slug:"/Methodology/User-Study-intro",permalink:"/Explainable-Ai-Comps-2024/Methodology/User-Study-intro",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Methodology/User-Study-intro.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ResNet",permalink:"/Explainable-Ai-Comps-2024/Methodology/ResNet"},next:{title:"LIME",permalink:"/Explainable-Ai-Comps-2024/category/lime"}},d={},h=[];function c(e){const t={h1:"h1",img:"img",p:"p",...(0,n.M)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"user-study-overview",children:"User Study Overview"}),"\n",(0,o.jsx)(t.p,{children:"A user study was designed around these explainability methods, which set out to assess these methods in a variety of ways. Loosely modeled after the anchoring paper (Ribeiro, 2016), the first component exposes respondents to a data point, and one explanatory method. To assess the explanatory power of the method, the respondent\u2019s accuracy at predicting the model\u2019s outcome in subsequent data points is assessed. If the explanation was effective at helping the respondent understand how the model makes its predictions, they should increase the respondent\u2019s ability to accurately predict the model. Following exposure to all the methods across a variety of samples, respondents provide feedback on each.\nThe respondents consist of [how many?] undergraduate or recent graduates with varying levels of familiarity with computer science and machine learning. Respondents also demonstrated a variety of opinions of ML going into the survey, from AI skeptics with humanities backgrounds to CS graduates working in the industry, developing AI. Our intention was to broadly assess user reactions to the different explanation methods, in terms of how much they effected user trust, and how easy they are to understand. Respondents were asked to comparatively assess the methods at the end of the survey."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/assets/20567330/d9c87adb-319f-47db-b43d-72fd47ae2885",alt:"image"})}),"\n",(0,o.jsx)(t.p,{children:"For a discussion of the MOC results, see --MOOC link--, and for ResNet, see --resnet link--."})]})}function l(e={}){const{wrapper:t}={...(0,n.M)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},2172:(e,t,s)=>{s.d(t,{I:()=>i,M:()=>r});var o=s(1504);const n={},a=o.createContext(n);function r(e){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);