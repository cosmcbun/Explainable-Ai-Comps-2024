"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[472],{4044:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>r,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>h});var n=i(7624),a=i(2172);const s={sidebar_position:1},o="What is XAI?",l={id:"Introduction/What is XAI",title:"What is XAI?",description:"Why should you trust Machine Learning (ML)? Headlines are filled with fundamental flaws in machine learning models published by technology giants.",source:"@site/docs/Introduction/What is XAI.md",sourceDirName:"Introduction",slug:"/Introduction/What is XAI",permalink:"/Explainable-Ai-Comps-2024/Introduction/What is XAI",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Introduction/What is XAI.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/Explainable-Ai-Comps-2024/category/introduction"},next:{title:"Introducing Our Project",permalink:"/Explainable-Ai-Comps-2024/Introduction/Introducing Our Project"}},r={},h=[{value:"Why XAI Matters",id:"why-xai-matters",level:2},{value:"Legal Implications",id:"legal-implications",level:3},{value:"Ethical Implications",id:"ethical-implications",level:3},{value:"ML Design Implications / For Programmers: why do we care?",id:"ml-design-implications--for-programmers-why-do-we-care",level:3},{value:"All of the Techniques illustrated on this Website are Model-Agnostic, Post Hoc, and Local.",id:"all-of-the-techniques-illustrated-on-this-website-are-model-agnostic-post-hoc-and-local",level:2},{value:"What Does Model-Agnostic Mean?",id:"what-does-model-agnostic-mean",level:3},{value:"What is Post Hoc Explanation?",id:"what-is-post-hoc-explanation",level:3},{value:"What is Local Interpretability?",id:"what-is-local-interpretability",level:3}];function c(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",...(0,a.M)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"what-is-xai",children:"What is XAI?"}),"\n",(0,n.jsxs)(t.p,{children:["Why should you trust Machine Learning (ML)? Headlines are filled with ",(0,n.jsx)(t.a,{href:"https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html",children:"fundamental flaws in machine learning models published by technology giants"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"Most real world applications lean away from inherently explainable models like decision trees or linear regression models, and towards black box models like neural networks due to their complex and robust hypothesis spaces."}),"\n",(0,n.jsx)(t.p,{children:"This page introduces the need for explainable AI (XAI) techniques. Explainable AI techniques attempt to illustrate how a machine learning model comes to a conclusion."}),"\n",(0,n.jsx)(t.h2,{id:"why-xai-matters",children:"Why XAI Matters"}),"\n",(0,n.jsxs)(t.p,{children:["We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences \u2014 namely their predictions. As discussed in Ribeiro et al. ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/1602.04938",children:"(2016, p.1)"}),", the act of explaining an AI\u2019s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users\u2019 trust in the model and exposing any possible errors in the model\u2019s structure."]}),"\n",(0,n.jsx)(t.h3,{id:"legal-implications",children:"Legal Implications"}),"\n",(0,n.jsxs)(t.p,{children:["With regulations like the EU\u2019s ",(0,n.jsx)(t.a,{href:"https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai",children:"Right to Explainability"})," and the United States\u2019 proposed ",(0,n.jsx)(t.a,{href:"https://www.whitehouse.gov/ostp/ai-bill-of-rights/",children:"AI Bill of Rights"}),", a machine learning model may no longer be a simple \u201cblack box\u201d: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche."]}),"\n",(0,n.jsxs)(t.p,{children:["In such an impossibly dense field, how can one quantify a method\u2019s efficacy? ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/Shapley%20Values/The%20EU's%20right%20to%20explainability",children:"Which method would a jury trust?"})]}),"\n",(0,n.jsx)(t.h3,{id:"ethical-implications",children:"Ethical Implications"}),"\n",(0,n.jsx)(t.p,{children:"High stakes examples of AI making decisions that people might want insight into (EX: bank loans and tumors)"}),"\n",(0,n.jsx)(t.p,{children:"==TODO: SAM=="}),"\n",(0,n.jsx)(t.h3,{id:"ml-design-implications--for-programmers-why-do-we-care",children:"ML Design Implications / For Programmers: why do we care?"}),"\n",(0,n.jsx)(t.p,{children:"==TODO: SAM=="}),"\n",(0,n.jsx)(t.p,{children:"insight into ML models\u2019 assumptions and priorities,\nidentifying blind spots, biases, and possible improvements,\ninfluencing model trust among users."}),"\n",(0,n.jsx)(t.h2,{id:"all-of-the-techniques-illustrated-on-this-website-are-model-agnostic-post-hoc-and-local",children:"All of the Techniques illustrated on this Website are Model-Agnostic, Post Hoc, and Local."}),"\n",(0,n.jsx)(t.h3,{id:"what-does-model-agnostic-mean",children:"What Does Model-Agnostic Mean?"}),"\n",(0,n.jsx)(t.p,{children:"Usage of a model-agnostic technique does not depend on knowledge of the specific AI model being analyzed. This means that, rather than looking at the specific activations of the network, or the weights and biases, we only have access to the outputs of the model. This may also be described as a black-box approach."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"Black Box Clipart",src:i(8868).c+"",title:"What is happening in the box? Not our problem!",width:"910",height:"380"})}),"\n",(0,n.jsxs)(t.p,{children:["There are a few reasons that this might be desirable. For instance, work done on one model can scale easily to others. With the same techniques in this project, we were able to analyze both ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/User%20Study/ResNet%20-%20Comparative%20Results",children:"images"})," and ",(0,n.jsx)(t.a,{href:"/Explainable-Ai-Comps-2024/User%20Study/MOOC%20-%20Comparative%20Results",children:"tabular data"}),"! Moreover, it allows us to gather insights into models we don't have unrestricted access to. Given that AI model weights are often a valuable property that companies have spent much money and time on tuning, they probably won't be willing to share the weights. But model-agnostic techniques only need to be able to query the model repeatedly."]}),"\n",(0,n.jsx)(t.h3,{id:"what-is-post-hoc-explanation",children:"What is Post Hoc Explanation?"}),"\n",(0,n.jsx)(t.p,{children:"Post hoc explanations attempt to answer, in retrospect, why the model chose the output or classification it did. It's beyond the intended scope of these techniques to predict what the model will say in future instances without testing it on those as well."}),"\n",(0,n.jsx)(t.h3,{id:"what-is-local-interpretability",children:"What is Local Interpretability?"}),"\n",(0,n.jsx)(t.p,{children:"These techniques are not focused on the behavior of the AI model as a whole, but rather on its behavior in a specific instance \u2013 one data point. The conclusions, thusly, cannot necessarily be extrapolated too far in any direction."}),"\n",(0,n.jsx)(t.h1,{id:"explainable-vs-interpretable-differentiation",children:"Explainable vs interpretable differentiation"}),"\n",(0,n.jsx)(t.p,{children:"==TODO: TOM=="})]})}function d(e={}){const{wrapper:t}={...(0,a.M)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8868:(e,t,i)=>{i.d(t,{c:()=>n});const n=i.p+"assets/images/Black_Box_Clipart-a5cb5e5cb2fc7a4a357f0d8e8523b664.png"},2172:(e,t,i)=>{i.d(t,{I:()=>l,M:()=>o});var n=i(1504);const a={},s=n.createContext(a);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);