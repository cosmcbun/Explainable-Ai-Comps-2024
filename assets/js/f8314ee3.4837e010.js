"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[8332],{2536:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>r,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>h});var a=s(7624),n=s(2172);const i={},o="Resnet: Comparative Results",l={id:"User Study/ResNet - Comparative Results",title:"Resnet: Comparative Results",description:"TL;DR",source:"@site/docs/User Study/ResNet - Comparative Results.md",sourceDirName:"User Study",slug:"/User Study/ResNet - Comparative Results",permalink:"/Explainable-Ai-Comps-2024/User Study/ResNet - Comparative Results",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/User Study/ResNet - Comparative Results.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"MOOC: Comparative Results",permalink:"/Explainable-Ai-Comps-2024/User Study/MOOC - Comparative Results"},next:{title:"User Study Overview",permalink:"/Explainable-Ai-Comps-2024/User Study/User-Study-(MOOC)"}},r={},h=[{value:"TL;DR",id:"tldr",level:2},{value:"Shapley Values",id:"shapley-values",level:2},{value:"Clashing explanations",id:"clashing-explanations",level:2}];function d(e){const t={blockquote:"blockquote",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,n.M)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"resnet-comparative-results",children:"Resnet: Comparative Results"}),"\n",(0,a.jsx)(t.h2,{id:"tldr",children:"TL;DR"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Shapley was the most trusted explanation technique and Anchor the least; but not by a substantial margin."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Explanations that better correlated with what people would've looked at themselves were recieved better."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"While experiance with AI was not strongly correlated with favorite explanaitons, those with more AI experiance were less willing to accept the differences between the explanations without worry."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"shapley-values",children:"Shapley Values"}),"\n",(0,a.jsx)(t.p,{children:"Shapley values, like in MOOC, were numerically ranked as the most trusted method. 8 of our 12 participants said they'd trust it to explain a model's thought process in new examples, as compared to 6 and 4 for LIME and Anchor respectively. Yet the praise of its visualizations was not universal. The heatmaps, at times, could look somewhat random, or cover up the image. As users put it,"}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:'\u201cShapley has more specificity, but sacrifices intuitiveness."'}),"\n",(0,a.jsx)(t.li,{children:'"If Shapley always had the edge in accuracy I would choose that but anchoring and lime have better display"'}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Some of this, admittedly, is hard to disentangle from the presentation of the results (as opposed to the techniques)."}),"\n",(0,a.jsx)(t.h2,{id:"clashing-explanations",children:"Clashing explanations"}),"\n",(0,a.jsx)(t.p,{children:"One phenomenon that confused participants was when the techniques gave contradictory explanations. One example of this is the picture of a dog that the model thought was a polar bear."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Figure 5",src:s(7084).c+"",title:"An image of a dog, with explanations for why it was predicted as a polar bear.",width:"1056",height:"816"})}),"\n",(0,a.jsx)(t.p,{children:'LIME focused on the dog\'s head, whereas Anchoring included most of the dog and much of the background but omitted much of the head, and Shapley highlighted some of the head red and some blue. All explanations are plausible in isolation, but one participant said "If I was given any single one I would say they help, but they confuse in total". After seeing all of the examples, another participant said "it is actively confusing to have different explanations for the same model. If they are seeing the same predictions, they should be showing the same explanation."'})]})}function c(e={}){const{wrapper:t}={...(0,n.M)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},7084:(e,t,s)=>{s.d(t,{c:()=>a});const a=s.p+"assets/images/polar_bear-slide-e238527e3059af9c15c839066d968eaa.jpg"},2172:(e,t,s)=>{s.d(t,{I:()=>l,M:()=>o});var a=s(1504);const n={},i=a.createContext(n);function o(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);