"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[4140],{36:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>h});var n=t(7624),i=t(2172);const o={"sidebar-position":3},l="Shapley's Math",s={id:"Shapley Values/Shapley's Math",title:"Shapley's Math",description:"\x3c!-- SOURCES:",source:"@site/docs/Shapley Values/Shapley's Math.md",sourceDirName:"Shapley Values",slug:"/Shapley Values/Shapley's Math",permalink:"/Explainable-Ai-Comps-2024/Shapley Values/Shapley's Math",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Shapley Values/Shapley's Math.md",tags:[],version:"current",frontMatter:{"sidebar-position":3},sidebar:"tutorialSidebar",previous:{title:"Shapley and Resnet",permalink:"/Explainable-Ai-Comps-2024/Shapley Values/Shapley and Resnet"},next:{title:"The EU's Right to Explainability",permalink:"/Explainable-Ai-Comps-2024/Shapley Values/The EU's right to explainability"}},r={},h=[{value:"Intuition - Game Theory",id:"intuition---game-theory",level:2},{value:"How do we simulate a feature before it joins the game?",id:"how-do-we-simulate-a-feature-before-it-joins-the-game",level:2},{value:"How to approximate Shapley values",id:"how-to-approximate-shapley-values",level:2},{value:"Basic properties",id:"basic-properties",level:2},{value:"Actually calculating a Shapley Value",id:"actually-calculating-a-shapley-value",level:2},{value:"Approximating the Shapley Value",id:"approximating-the-shapley-value",level:2}];function p(e){const a={a:"a",em:"em",h1:"h1",h2:"h2",p:"p",...(0,i.M)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h1,{id:"shapleys-math",children:"Shapley's Math"}),"\n",(0,n.jsx)(a.p,{children:"Due to their strong mathematical backing, Shapley values are incredibly widely used in the field and have some of the most variation among explanation techniques, thus making them almost obligatory to include in the project. But how do Shapley values work? "}),"\n",(0,n.jsx)(a.h2,{id:"intuition---game-theory",children:"Intuition - Game Theory"}),"\n",(0,n.jsx)(a.p,{children:"Before we wade into the math, let's establish a quick base: a machine-learning model takes a set of features as input, performs some kind of calculation on them, and returns an output (In our case, a real-valued confidence score in a potential classification). Now that we have this foundation, let's begin."}),"\n",(0,n.jsxs)(a.p,{children:["Shapley values have their roots in ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Cooperative_game_theory",children:"cooperative game theory"}),". Assume that our machine learning model is a game, where each feature's value is a player, and where the model's output is the final result of the game. Shapley values tell us how each player contributed to the final result of the game."]}),"\n",(0,n.jsx)(a.p,{children:"Suppose each feature values begins to contribute to the game in a random order. The Shapley value for a feature tells you how much they change the game's result when they start to play along side everyone else."}),"\n",(0,n.jsx)(a.h2,{id:"how-do-we-simulate-a-feature-before-it-joins-the-game",children:"How do we simulate a feature before it joins the game?"}),"\n",(0,n.jsxs)(a.p,{children:["In Shapley's original text, ",(0,n.jsx)(a.a,{href:"https://www.rand.org/content/dam/rand/pubs/papers/2021/P295.pdf",children:(0,n.jsx)(a.em,{children:"A value for n-person games"})}),", a player can simply not join the game until their turn; however, a machine learning model most often takes a fixed number of inputs, and as such a player simply cannot choose to refrain from partaking in the game."]}),"\n",(0,n.jsxs)(a.p,{children:["Therefore, we must find a way to pretend that a player is absent. This is one of two places where approaches diverge. In some cases, a value is randomly sampled from an acceptable range or even completely unconstrainedly. Our implementation, in accordance with ",(0,n.jsx)(a.a,{href:"https://christophm.github.io/interpretable-ml-book/shapley.html#:~:text=It%20is%20not%20sufficient%20to%20access%20the%20prediction%20function%20because%20you%20need%20the%20data%20to%20replace%20parts%20of%20the%20instance%20of%20interest%20with%20values%20from%20randomly%20drawn%20instances%20of%20the%20data.",children:"Cristoph Molnar's implementation"}),', accesses a random instance from our training data to replace any values which are "not playing."']}),"\n",(0,n.jsx)(a.h2,{id:"how-to-approximate-shapley-values",children:"How to approximate Shapley values"}),"\n",(0,n.jsx)(a.h2,{id:"basic-properties",children:"Basic properties"}),"\n",(0,n.jsx)(a.h2,{id:"actually-calculating-a-shapley-value",children:"Actually calculating a Shapley Value"}),"\n",(0,n.jsx)(a.h2,{id:"approximating-the-shapley-value",children:"Approximating the Shapley Value"})]})}function u(e={}){const{wrapper:a}={...(0,i.M)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},2172:(e,a,t)=>{t.d(a,{I:()=>s,M:()=>l});var n=t(1504);const i={},o=n.createContext(i);function l(e){const a=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),n.createElement(o.Provider,{value:a},e.children)}}}]);