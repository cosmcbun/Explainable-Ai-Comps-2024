---
sidebar_position: 2
---


# Why XAI Matters

We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences — namely their predictions. As discussed in Ribeiro et al. (2016, p.1), the act of explaining an AI’s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users’ trust in the model and exposing any possible errors in the model’s structure. With regulations like the EU’s [Right to Explainability](https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai) and the United States’ proposed [AI Bill of Rights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/), a machine learning model may no longer be a simple “black box”: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche. In such an impossibly dense field, how can one quantify a method’s efficacy? Which method would a jury trust?

## Our Vision

Through this project, we explore three major avenues for model explainability across two contrasting domains of machine-learning tasks (ResNet and MOOC). Namely, we will be apply Shapley, LIME, and Anchoring to two separate models of unique architecture which specialize in classification based on tabular and image data, respectively. On this site, you will find a comprehensive analysis of each method’s approach, what they highlight, and how they compare to the others. We discuss the literature surrounding these methods, and we compare their performances, including a user study. 

This website, analysis, and survey serves to gauge the public’s perception of each method.
