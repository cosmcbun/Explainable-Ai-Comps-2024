"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[996],{2796:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>h,contentTitle:()=>s,default:()=>u,frontMatter:()=>n,metadata:()=>r,toc:()=>c});var a=i(7624),o=i(2172);const n={sidebar_position:3},s="Anchoring",r={id:"XAI Techniques/Anchors",title:"Anchoring",description:"Ribeiro, Singh, and Guestrin, the original authors of LIME, also created anchoring as an Explainable AI model, which has some similarities with LIME, but outputs its explanations in a different form. Like LIME, Anchoring involves perturbing the data point in question to see how the results from the black box change. Ribeiro et al. define their anchor like so: \u201cAn anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally \u2013 such that changes to the rest of the feature values of the instance do not matter\u201d (Ribeiro et al, 2018). For example, when evaluating the sentiment of the phrase \u201cEverything is bad\u201d, we would say that the word \u201cbad\u201d tells us that the sentiment is negative, making \u201cbad\u201d our anchor. However, if we changed the phrase to \u201cEverything is not bad\u201d, \u201cbad\u201d would not make a good anchor anymore, because the sentiment of this phrase is positive. Our new anchor would be \u201cnot bad\u201d, because these are the words that determine that the phrase is positive. The same principle can be used for multiple types of data, not just textual data. For tabular data, the algorithm will seek to find which feature values were the most important in coming to a particular decision, and for image classification the anchor will be a set of superpixels that have the most importance in determining the prediction of the model. A clear advantage of anchoring is that its output is intuitive and easy to interpret (\u201cthe sentiment of this sentence is positive because it contains the words \u2018not bad\u2019\u201d), as opposed to being a sea of coefficients.",source:"@site/docs/XAI Techniques/Anchors.md",sourceDirName:"XAI Techniques",slug:"/XAI Techniques/Anchors",permalink:"/Explainable-Ai-Comps-2024/XAI Techniques/Anchors",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/XAI Techniques/Anchors.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Shapley Values",permalink:"/Explainable-Ai-Comps-2024/XAI Techniques/Shapley"},next:{title:"User Study",permalink:"/Explainable-Ai-Comps-2024/User Study"}},h={},c=[];function l(e){const t={h1:"h1",p:"p",...(0,o.M)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"anchoring",children:"Anchoring"}),"\n",(0,a.jsx)(t.p,{children:"Ribeiro, Singh, and Guestrin, the original authors of LIME, also created anchoring as an Explainable AI model, which has some similarities with LIME, but outputs its explanations in a different form. Like LIME, Anchoring involves perturbing the data point in question to see how the results from the black box change. Ribeiro et al. define their anchor like so: \u201cAn _anchor _explanation is a rule that sufficiently \u201canchors\u201d the prediction locally \u2013 such that changes to the rest of the feature values of the instance do not matter\u201d (Ribeiro et al, 2018). For example, when evaluating the sentiment of the phrase \u201cEverything is bad\u201d, we would say that the word \u201cbad\u201d tells us that the sentiment is negative, making \u201cbad\u201d our anchor. However, if we changed the phrase to \u201cEverything is not bad\u201d, \u201cbad\u201d would not make a good anchor anymore, because the sentiment of this phrase is positive. Our new anchor would be \u201cnot bad\u201d, because these are the words that determine that the phrase is positive. The same principle can be used for multiple types of data, not just textual data. For tabular data, the algorithm will seek to find which feature values were the most important in coming to a particular decision, and for image classification the anchor will be a set of superpixels that have the most importance in determining the prediction of the model. A clear advantage of anchoring is that its output is intuitive and easy to interpret (\u201cthe sentiment of this sentence is positive because it contains the words \u2018not bad\u2019\u201d), as opposed to being a sea of coefficients."}),"\n",(0,a.jsx)(t.p,{children:"When looking for an anchor, there are two main attributes that we are looking for. One is precision. We want to choose an anchor that we are pretty sure is correct, meaning that it does contribute heavily to the prediction of the black box model. The other is coverage, meaning that our anchor still holds for as big of a subset of the input space as possible, which in practice generally means trying to make the anchor more concise. The algorithm optimizes this by setting a precision threshold, and choosing the anchor above this precision with the highest coverage. In order to generate this anchor, we start off with a null rule, and then iteratively add features to it until we have a rule that meets the precision threshold we are looking for. We start off with multiple possible rules to make sure we search a reasonably sized portion of the sample space while keeping the algorithm computationally feasible (Ribeiro et al, 2018). One weakness of anchoring is the difficulty of finding a distribution within which to perturb data (the same problem shows up in LIME and is somewhat inherent to perturbation based algorithms). Another issue is that there is no good anchor for some data points, especially ones near decision boundaries or belonging to fringe classes."})]})}function u(e={}){const{wrapper:t}={...(0,o.M)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},2172:(e,t,i)=>{i.d(t,{I:()=>r,M:()=>s});var a=i(1504);const o={},n=a.createContext(o);function s(e){const t=a.useContext(n);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(n.Provider,{value:t},e.children)}}}]);