<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-XAI+AdvAI" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Adversarial Explanations | Explainable AI: Breaking Down the Black Box</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/XAI+AdvAI"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Adversarial Explanations | Explainable AI: Breaking Down the Black Box"><meta data-rh="true" name="description" content="Although Explainable AI can exist in a vacuum, it does not have to. As noted in our introduction, XAI can be used to motivate and direct the design of models that users can trust it strives to &quot;cause the black box to break down.&quot; Both of these approaches to AI share a common goal: to construct models that are more robust against variations in data, biases, and even active exploitation."><meta data-rh="true" property="og:description" content="Although Explainable AI can exist in a vacuum, it does not have to. As noted in our introduction, XAI can be used to motivate and direct the design of models that users can trust it strives to &quot;cause the black box to break down.&quot; Both of these approaches to AI share a common goal: to construct models that are more robust against variations in data, biases, and even active exploitation."><link data-rh="true" rel="icon" href="/Explainable-Ai-Comps-2024/img/carleton-enhanced.ico"><link data-rh="true" rel="canonical" href="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/XAI+AdvAI"><link data-rh="true" rel="alternate" href="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/XAI+AdvAI" hreflang="en"><link data-rh="true" rel="alternate" href="https://cosmcbun.github.io/Explainable-Ai-Comps-2024/XAI+AdvAI" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/Explainable-Ai-Comps-2024/assets/css/styles.0e5b6101.css">
<script src="/Explainable-Ai-Comps-2024/assets/js/runtime~main.4207c538.js" defer="defer"></script>
<script src="/Explainable-Ai-Comps-2024/assets/js/main.6c6cf8e7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Explainable-Ai-Comps-2024/"><div class="navbar__logo"><img src="/Explainable-Ai-Comps-2024/img/carleton logo enhanced.png" alt="Carleton College Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Explainable-Ai-Comps-2024/img/carleton logo enhanced.png" alt="Carleton College Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Explainable AI: Breaking Down the Black Box</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Explainable-Ai-Comps-2024/category/introduction">Writeup</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/Explainable-Ai-Comps-2024/About Us">About Us</a><a href="https://github.com/cosmcbun/Explainable-Ai-Comps-2024/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://cs.carleton.edu/cs_comps/2324/explainable-ai/index.php" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Project Description<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/introduction">Introduction</a><button aria-label="Expand sidebar category &#x27;Introduction&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/methodology">Methodology</a><button aria-label="Expand sidebar category &#x27;Methodology&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/lime">LIME</a><button aria-label="Expand sidebar category &#x27;LIME&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/shapley-values">Shapley Values</a><button aria-label="Expand sidebar category &#x27;Shapley Values&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/anchors">Anchors</a><button aria-label="Expand sidebar category &#x27;Anchors&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Explainable-Ai-Comps-2024/Tumors">Tumors Case Study</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Explainable-Ai-Comps-2024/category/user-study--results">User Study / Results</a><button aria-label="Expand sidebar category &#x27;User Study / Results&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Explainable-Ai-Comps-2024/Posters">Posters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Explainable-Ai-Comps-2024/About Us">About Us</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Explainable-Ai-Comps-2024/Citations">Bibliography</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Explainable-Ai-Comps-2024/XAI+AdvAI">Adversarial Explanations</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Explainable-Ai-Comps-2024/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Adversarial Explanations</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Adversarial Explanations</h1>
<p>Although Explainable AI can exist in a vacuum, it does not have to. As noted in <a href="/Explainable-Ai-Comps-2024/Introduction/What is XAI">our introduction</a>, XAI can be used to motivate and direct the design of models that users can trust: it &quot;breaks down the black box.&quot; Similarly, Adversarial Artificial Intelligence strives to find how a machine learning model can be exploited: it strives to &quot;cause the black box to break down.&quot; Both of these approaches to AI share a common goal: to construct models that are more robust against variations in data, biases, and even active exploitation.</p>
<p>As such, we in the XAI project have partnered with Jonas Bartels, Alice Cutter, Sriya Konda, Yuxin Lin, Sky Lu, and Tingjun Tu of Carleton College&#x27;s <a href="/Explainable-Ai-Comps-2024/">Adversarial Artificial Intelligence project</a>
to bring you <em>Adversarial Explanations: an Exploration and Exploitation of Machine Thought</em>. In this section, we will present three examples of adversarial attacks on the <code>ResNet50</code> image classifier. All attacks are carried out through black-box methods, meaning that they do not have access to any parts of our model, and all explanations are model-agnostic, meaning that they do not require a specific architecture.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="local-search-adversary">Local Search Adversary<a href="#local-search-adversary" class="hash-link" aria-label="Direct link to Local Search Adversary" title="Direct link to Local Search Adversary">​</a></h2>
<blockquote>
<p>The Local Search adversary utilizes the concept of greedy local search. It iteratively selects a small set of pixels to perturb that eventually would cause misclassification by a deep neural network without using any gradient information. <br>
- Tingjun Tu, <em>An Exploration of Adversarial Attacks on Image Classifiers</em></p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="Figure 1" src="/Explainable-Ai-Comps-2024/assets/images/LSA-1e14465cd08b312a8a28c33f8caa0b9e.png" title="Pre-perturbation and post-perturbation of an image of a Bengal cat using a Local Search adversary, each accompanied by three explanation techniques." width="3000" height="2063" class="img_ev3q"></p>
<p>Before perturbation, our classifier correctly predicts that this is a Bengal cat with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>63.1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">63.1\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">63.1%</span></span></span></span> confidence. All of our techniques agree that the model looked at the area immediately around the cat to classify it as such. However, LIME and Anchors propose that our model also focuses a portion of the stairway to achieve this result, to which Shapley does not agree.</p>
<p>After perturbation, our model becomes much more hectic, producing a prediction of “altar.” Each of our techniques confirms this messiness, as we can see that the model must now look at more than half of the image to find a modicum of justification for this prediction. This hectic nature is most likely due to an utter lack of confidence, which is displayed through Shapley values: although it is not in this image, the deepest red on this graph only adds <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mn>0.15</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">+0.15\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">0.15%</span></span></span></span> confidence to the model from base. It seems as though the machine learning model, as it is starved for foreground artifacts by LSA&#x27;s pixel-injections, begins to search elsewhere for any possible indication of class; therefore, upon finding the cat&#x27;s sloping surroundings, it predicts &quot;altar.&quot;</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="fast-gradient-sign-method-fsgm">Fast Gradient Sign Method (FSGM)<a href="#fast-gradient-sign-method-fsgm" class="hash-link" aria-label="Direct link to Fast Gradient Sign Method (FSGM)" title="Direct link to Fast Gradient Sign Method (FSGM)">​</a></h2>
<blockquote>
<p>FSGM uses the gradient of a loss function with respect to the input data to perturb each pixel in the direction that lowers the confidence of the correct prediction by a distance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ϵ</span></span></span></span>.
<br> - Jonas Bartels, <em>An Exploration of Adversarial Attacks on Image Classifiers</em></p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="Figure 2" src="/Explainable-Ai-Comps-2024/assets/images/FGSM-fb680cc6e389e2f48d7c39c37b940908.png" title="Pre-perturbation and post-perturbation of an image of a German Shorthaired Pointer dog using the Fast Gradient Sign Method, each accompanied by three explanation techniques." width="3000" height="2063" class="img_ev3q"></p>
<p>Pre-adversary, the model correctly predicts that this is a German Shorthair Pointer with 89.4% confidence. All of our explanations seem to agree that the dog itself is the point of focus. The Shapley values argue that our model looks for more specific parts of the dog, such as its face and legs, while more ambiguous parts of the body are unfavorably considered.</p>
<p>Here, the attack causes our German Shorthaired Pointer to be misclassified as a Vizsla, a similar dog breed. None of the explanations here are able to agree on the model’s decision. First, LIME claims that our model looks to the head of the dog, while Anchors and Shapley values argue that most if not all of the dog is observed in the prediction. From here, LIME and Anchors agree that some portion of the background is present in the prediction, while Shapley values argue that the model predicts almost exclusively on the dog’s main body. Perhaps there are a disproportionate number of Vizsla dog images with similar backgrounds, such that obscuring the original breed causes the model to be more desperate for any predicable information, or it may be caused by the simple visual similarities between the two.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="projected-gradient-descent">Projected Gradient Descent<a href="#projected-gradient-descent" class="hash-link" aria-label="Direct link to Projected Gradient Descent" title="Direct link to Projected Gradient Descent">​</a></h2>
<blockquote>
<p>Projected Gradient Descent calculates the gradient of loss for an input image and applies it as a perturbation to the image over a course of small steps, modifying hyperparameters to keep the image looking as normal as possible. <br>
- Sriya Konda, <em>An Exploration of Adversarial Attacks on Image Classifiers</em></p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="Figure 3" src="/Explainable-Ai-Comps-2024/assets/images/PGD-6b8fbf0adcaf7d3f803bc46df9bd1886.png" title="Pre-perturbation and post-perturbation of an image of a Persian cat using Projected Gradient Descent, each accompanied by three explanation techniques." width="1600" height="1100" class="img_ev3q"></p>
<p>This is our hardest image to break, as the model correctly classifies this Persian cat with 99.1% confidence. All three techniques portray the strength of its reasoning, as all highlight the face and upper body of the cat almost exclusively, with some background artifacts.</p>
<p>Although perturbation of this sample does not succeed in its reclassification, the confidence drops to 26.2%. Moreover, this modification has precipitated the most interesting explanations of these samples: Shapley values propose that the cat is still the focus of the model for this prediction (only adding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mn>0.3</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">+0.3\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">0.3%</span></span></span></span> per pixel at most), while LIME argues that the model must avail of the background to achieve this result. Finally, Anchors show no change in the input image. Due to this technique’s niche <a href="/Explainable-Ai-Comps-2024/Anchors/Introduction">(see our Anchoring page)</a>, we can infer that since no data may be removed from the image, no anchors are present in the input, and thus no specific piece of the input may be identified as sufficient for the model to predict this class. Such a perturbation in this image may have obscured the cat just enough to lower the confidence score and to cause the model to begin to reach out; however, upon looking for other features, the austere red background may not have fit any of its other filters, disallowing alternate predictions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Adversarial AI poses a threat to models and can facilitate the bolstering of models, but when paired with explanations, we can see more easily why a model breaks and exactly where why we should or should not trust its decision. Identically, Explainable AI can display potential biases in machine learning models, allowing architects to improve their robustness, but an explanation cannot account for exploitable biases that are not visible in the data, thus requiring an adversary to reveal holes in the machine&#x27;s thought processes.</p>
<p>Adversarial AI and Explainable AI are a part of a rich ecosystem of model fortification, each of which fits a niche alone; however, these two methods combined are able to build a cycle of continuous improvement, that which will ultimately create a safer and more trustworthy model.</p>
<br>
<p><em>We are grateful the Adversarial AI group for making this collaboration possible, and we would like to extend special thanks to Alice Cutter and Tingjun Tu for working so closely with us.</em></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/XAI+AdvAI.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Explainable-Ai-Comps-2024/Citations"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Bibliography</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#local-search-adversary" class="table-of-contents__link toc-highlight">Local Search Adversary</a></li><li><a href="#fast-gradient-sign-method-fsgm" class="table-of-contents__link toc-highlight">Fast Gradient Sign Method (FSGM)</a></li><li><a href="#projected-gradient-descent" class="table-of-contents__link toc-highlight">Projected Gradient Descent</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Our Project</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/Introduction">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/Methodology">Methodology</a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/user-study--results">User Study</a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/Citations">Citations</a></li></ul></div><div class="col footer__col"><div class="footer__title">XAI Techniques</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/LIME">LIME</a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/Shapley-values">Shapley values</a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/category/Anchors">Anchors</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://cs.carleton.edu/cs_comps/2324/explainable-ai/index.php" target="_blank" rel="noopener noreferrer" class="footer__link-item">Project Description<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/cosmcbun/Explainable-Ai-Comps-2024/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Repository<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/Explainable-Ai-Comps-2024/About Us">About Us</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Carleton College XAI Group | Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>