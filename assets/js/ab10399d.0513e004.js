"use strict";(self.webpackChunkxai_carleton_comps_2024=self.webpackChunkxai_carleton_comps_2024||[]).push([[9300],{3372:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>h,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var n=t(7624),o=t(2172);const a={sidebar_position:2},s="Why XAI Matters",r={id:"Introduction/Vision",title:"Why XAI Matters",description:"We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences \u2014 namely their predictions. As discussed in Ribeiro et al. (2016, p.1), the act of explaining an AI\u2019s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users\u2019 trust in the model and exposing any possible errors in the model\u2019s structure. With regulations like the EU\u2019s Right to Explainability and the United States\u2019 proposed AI Bill of Rights, a machine learning model may no longer be a simple \u201cblack box\u201d: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche. In such an impossibly dense field, how can one quantify a method\u2019s efficacy? Which method would a jury trust?",source:"@site/docs/Introduction/Vision.md",sourceDirName:"Introduction",slug:"/Introduction/Vision",permalink:"/Explainable-Ai-Comps-2024/Introduction/Vision",draft:!1,unlisted:!1,editUrl:"https://github.com/cosmcbun/Explainable-Ai-Comps-2024/tree/main/Website/XAI-Carleton-Comps-2024/docs/Introduction/Vision.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"What is XAI?",permalink:"/Explainable-Ai-Comps-2024/Introduction/What is XAI"},next:{title:"What kind of techniques did we Study?",permalink:"/Explainable-Ai-Comps-2024/Introduction/What kind of techniqniques did we study"}},h={},c=[{value:"Our Vision",id:"our-vision",level:2}];function l(e){const i={a:"a",h1:"h1",h2:"h2",p:"p",...(0,o.M)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"why-xai-matters",children:"Why XAI Matters"}),"\n",(0,n.jsxs)(i.p,{children:["We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences \u2014 namely their predictions. As discussed in Ribeiro et al. (2016, p.1), the act of explaining an AI\u2019s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users\u2019 trust in the model and exposing any possible errors in the model\u2019s structure. With regulations like the EU\u2019s ",(0,n.jsx)(i.a,{href:"https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai",children:"Right to Explainability"})," and the United States\u2019 proposed ",(0,n.jsx)(i.a,{href:"https://www.whitehouse.gov/ostp/ai-bill-of-rights/",children:"AI Bill of Rights"}),", a machine learning model may no longer be a simple \u201cblack box\u201d: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche. In such an impossibly dense field, how can one quantify a method\u2019s efficacy? Which method would a jury trust?"]}),"\n",(0,n.jsx)(i.h2,{id:"our-vision",children:"Our Vision"}),"\n",(0,n.jsx)(i.p,{children:"Through this project, we explore three major avenues for model explainability across two contrasting domains of machine-learning tasks (ResNet and MOOC). Namely, we will be apply Shapley, LIME, and Anchoring to two separate models of unique architecture which specialize in classification based on tabular and image data, respectively. On this site, you will find a comprehensive analysis of each method\u2019s approach, what they highlight, and how they compare to the others. We discuss the literature surrounding these methods, and we compare their performances, including a user study."})]})}function d(e={}){const{wrapper:i}={...(0,o.M)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},2172:(e,i,t)=>{t.d(i,{I:()=>r,M:()=>s});var n=t(1504);const o={},a=n.createContext(o);function s(e){const i=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);