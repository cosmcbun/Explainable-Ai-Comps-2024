---
sidebar_position: 1
---

# Xhat is XAI?

We are living through a revolution of the standards for ethical machine learning practices which has been thoroughly marked by the need to explain artificial intelligences — namely their predictions. As discussed in Ribeiro et al. (2016, p.1), the act of explaining an AI’s prediction presents the audience with visualizations pertaining to the actions it made to achieve such a decision, thus building users’ trust in the model and exposing any possible errors in the model’s structure. With regulations like the EU’s _Right to Explainability_ and the United States’ proposed _AI Bill of Rights_, a machine learning model may no longer be a simple “black box”: in order to prevent criminal charges, the creators of high-impact models must be able to justify each of their predictions. Over the past few years, the field has thus become inundated with approaches, each a bid for its own niche. In such an impossibly dense field, how can one quantify a method’s efficacy? Which method would a jury trust?

Through this project, we will be exploring three major avenues for model explainability across two contrasting domains of machine-learning tasks (ResNet and MOOC). Namely, we will be applying Shapley, LIME, and Anchoring to two separate models of unique architecture which specialize in classification based on tabular and image data, respectively. This will culminate in a website which houses a comprehensive analysis of each method’s approach highlights and how they compare to the others, discussing the literature surrounding them and benchmarks of their performances, including a user study of Carleton College students of varying technical backgrounds. This study, inspired by Ribeiro et al. (2018), will ask users to predict alongside a model after being shown various amounts and methods of explanation. This paper, alongside these analyses serves to gauge the public’s perception of each method.


## Deliverables


It is of primary importance that the group understands our three explainable AI methods. This necessitates a literature review, with papers and articles that introduce and implement Shapley and Anchor/LIME. After reading about them, we also want to implement them on complex existing models like ResNet. While the models will mostly be imported, this code base connecting existing models with existing XAI libraries will be another deliverable for our final project.

However, we don’t just want to understand these methods for our own oral exams’ sakes, but also so we can explain them to other students and get their feedback. We want to have a user study, headed by Josh, Tom, Chris and Sam, to lead this effort. With examples pulled from our data (like annotated images) that can be presented to peers for their feedback, we will ask about how compelling they find the explanation (versus the other XAI techniques), what could be improved, and whether it inspires confidence in them regarding the model as a whole. The product of this user study – whether in the form of notes, recordings, or filled-out surveys, would be another deliverable. While we shouldn’t need to recruit many students, depending on how many questions we want to ask, we will also need some resources to encourage participation. 10 participants at 45 minutes each and $12.5/hour – $100 in total – should be more than enough for this purpose.

After the user study, we will need two more deliverables: posters, for the poster session, and a website to show our results (and maybe even allow users to play with the models themselves, if we have time). The website push will be led by Lev and Adrian, at least until the other components of the project are completed. In a successful version of this project, we will be able to clearly explain the differences between these tools to anyone who comes to our poster sessions or website, along with data gathered from their peers about the qualitative efficacy of each technique.